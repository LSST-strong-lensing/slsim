{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation-based Inference Tutorial\n",
    "\n",
    "**Description:** In this tutorial, we will go over a demonstration of using `slsim`-simulated strong lens images to perform simulation-based inference (SBI), specifically neural posterior estimation (NPE), with the `sbi` package.\n",
    "\n",
    "**Author:** Steven Dillmann\n",
    "\n",
    "**Date:** 9 Aug 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Installation ===\n",
    "\n",
    "%pip install sbi\n",
    "%pip install tarp\n",
    "%pip install deprecation\n",
    "%pip install corner\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "\n",
    "# Basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# SBI\n",
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import CNNEmbedding\n",
    "from sbi.utils import MultipleIndependent\n",
    "from tarp import get_tarp_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading ===\n",
    "\n",
    "# TODO: Change this to the path to the data folder on your machine\n",
    "data_folder_path = \"/Users/steven/Desktop/slsim/sbi_data/\"\n",
    "\n",
    "# Functions to load image data and labels/targets\n",
    "def load_image_data(file_path: str) -> torch.Tensor:\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = f[\"data\"][:] \n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    return data_tensor\n",
    "\n",
    "def load_parameter_labels(file_path: str, keep_columns: list) -> torch.Tensor:\n",
    "    parameters = pd.read_csv(file_path)\n",
    "    parameters = parameters[keep_columns]\n",
    "    parameters_tensor = torch.tensor(parameters.values, dtype=torch.float32)\n",
    "    return parameters_tensor\n",
    "\n",
    "# Load the training and test image data\n",
    "train_data_path = os.path.join(data_folder_path, \"train\", \"image_data.h5\")\n",
    "test_data_path = os.path.join(data_folder_path, \"test\", \"image_data.h5\")\n",
    "train_data = load_image_data(train_data_path)\n",
    "test_data = load_image_data(test_data_path)\n",
    "print(\"Training Data Shape:\", train_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "\n",
    "# Load the training and test labels/targets and select the parameters you want to infer\n",
    "parameters_to_infer = [\n",
    "    \"main_deflector_parameters_theta_E\", # Einstein radius\n",
    "    \"main_deflector_parameters_gamma1\", # gamma1\n",
    "    \"main_deflector_parameters_gamma2\", # gamma2\n",
    "    \"main_deflector_parameters_gamma\", # gamma_lens\n",
    "    \"main_deflector_parameters_e1\", # e1\n",
    "    \"main_deflector_parameters_e2\", # e2\n",
    "    \"main_deflector_parameters_center_x\", # x_lens\n",
    "    \"main_deflector_parameters_center_y\", # y_lens\n",
    "    \"source_parameters_center_x\", # x_source\n",
    "    \"source_parameters_center_y\" # y_source\n",
    "    ]\n",
    "parameter_names = [r'$\\theta_\\mathrm{E}$',r'$\\gamma_1$',r'$\\gamma_2$',r'$\\gamma_\\mathrm{lens}$',r'$e_1$',\n",
    "               r'$e_2$',r'$x_\\mathrm{lens}$',r'$y_\\mathrm{lens}$',r'$x_\\mathrm{src}$',r'$y_\\mathrm{src}$']\n",
    "\n",
    "train_labels_path = os.path.join(data_folder_path, \"train\", \"metadata.csv\")\n",
    "test_targets_path = os.path.join(data_folder_path, \"test\", \"metadata.csv\")\n",
    "train_theta = load_parameter_labels(train_labels_path, parameters_to_infer)\n",
    "test_theta = load_parameter_labels(test_targets_path, parameters_to_infer)\n",
    "print(\"Training Labels Shape:\", train_theta.shape)\n",
    "print(\"Test Targets Shape:\", test_theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Neural Network ===\n",
    "\n",
    "# Define embedding network\n",
    "embedding_net = CNNEmbedding(\n",
    "    input_shape=(180, 180),                 # Input shape of the images\n",
    "    in_channels=1,                          # Number of channels in the input images\n",
    "    out_channels_per_layer=[16, 32, 64],    # Number of channels in each layer\n",
    "    num_conv_layers=3,                      # Number of convolutional layers\n",
    "    num_linear_layers=2,                    # Number of linear layers\n",
    "    output_dim=32,                          # Output dimension of the embedding network\n",
    "    kernel_size=3,                          # Kernel size of the convolutional layers\n",
    "    pool_kernel_size=2                      # Kernel size of the pooling layers\n",
    ")\n",
    "\n",
    "# Define density estimator\n",
    "density_estimator= posterior_nn(\n",
    "    model='maf',                           # Model type: 'maf', 'nsf', 'mdn', etc.\n",
    "    hidden_features=32,                    # Number of hidden features in the MAF\n",
    "    num_transforms=10,                     # Number of transformations in the MAF\n",
    "    embedding_net=embedding_net            # Embedding network\n",
    ")\n",
    "\n",
    "# Define the NPE inference object\n",
    "npe = NPE(density_estimator=density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slsim-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
