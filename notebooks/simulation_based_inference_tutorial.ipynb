{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation-based Inference Tutorial\n",
    "\n",
    "**Description:** In this tutorial, we will go over a demonstration of using `slsim`-simulated strong lens images to perform simulation-based inference (SBI), specifically neural posterior estimation (NPE), with the `sbi` package.\n",
    "\n",
    "**Author:** Steven Dillmann\n",
    "\n",
    "**Date:** 9 Aug 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Installation ===\n",
    "\n",
    "%pip install sbi\n",
    "%pip install tarp\n",
    "%pip install deprecation\n",
    "%pip install corner\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "\n",
    "# Basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# SBI\n",
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import CNNEmbedding\n",
    "from sbi.utils import MultipleIndependent\n",
    "from tarp import get_tarp_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading ===\n",
    "\n",
    "# TODO: Change this to the path to the data folder on your machine\n",
    "data_folder_path = \"/Users/steven/Desktop/slsim/sbi_data/\"\n",
    "\n",
    "# Functions to load image data and labels/targets\n",
    "def load_image_data(file_path: str) -> torch.Tensor:\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = f[\"data\"][:] \n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    return data_tensor\n",
    "\n",
    "def load_parameter_labels(file_path: str, keep_columns: list) -> torch.Tensor:\n",
    "    parameters = pd.read_csv(file_path)\n",
    "    parameters = parameters[keep_columns]\n",
    "    parameters_tensor = torch.tensor(parameters.values, dtype=torch.float32)\n",
    "    return parameters_tensor\n",
    "\n",
    "# Load the training and test image data\n",
    "train_data_path = os.path.join(data_folder_path, \"train\", \"image_data.h5\")\n",
    "test_data_path = os.path.join(data_folder_path, \"test\", \"image_data.h5\")\n",
    "train_data = load_image_data(train_data_path)\n",
    "test_data = load_image_data(test_data_path)\n",
    "print(\"Training Data Shape:\", train_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "\n",
    "# Load the training and test labels/targets and select the parameters you want to infer\n",
    "parameters_to_infer = [\n",
    "    \"main_deflector_parameters_theta_E\", # Einstein radius\n",
    "    \"main_deflector_parameters_gamma1\", # gamma1\n",
    "    \"main_deflector_parameters_gamma2\", # gamma2\n",
    "    \"main_deflector_parameters_gamma\", # gamma_lens\n",
    "    \"main_deflector_parameters_e1\", # e1\n",
    "    \"main_deflector_parameters_e2\", # e2\n",
    "    \"main_deflector_parameters_center_x\", # x_lens\n",
    "    \"main_deflector_parameters_center_y\", # y_lens\n",
    "    \"source_parameters_center_x\", # x_source\n",
    "    \"source_parameters_center_y\" # y_source\n",
    "    ]\n",
    "parameter_names = [r'$\\theta_\\mathrm{E}$',r'$\\gamma_1$',r'$\\gamma_2$',r'$\\gamma_\\mathrm{lens}$',r'$e_1$',\n",
    "               r'$e_2$',r'$x_\\mathrm{lens}$',r'$y_\\mathrm{lens}$',r'$x_\\mathrm{src}$',r'$y_\\mathrm{src}$']\n",
    "\n",
    "train_labels_path = os.path.join(data_folder_path, \"train\", \"metadata.csv\")\n",
    "test_targets_path = os.path.join(data_folder_path, \"test\", \"metadata.csv\")\n",
    "train_theta = load_parameter_labels(train_labels_path, parameters_to_infer)\n",
    "test_theta = load_parameter_labels(test_targets_path, parameters_to_infer)\n",
    "print(\"Training Labels Shape:\", train_theta.shape)\n",
    "print(\"Test Targets Shape:\", test_theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Neural Network ===\n",
    "\n",
    "# Define embedding network\n",
    "embedding_net = CNNEmbedding(\n",
    "    input_shape=(180, 180),                 # Input shape of the images\n",
    "    in_channels=1,                          # Number of channels in the input images\n",
    "    out_channels_per_layer=[16, 32, 64],    # Number of channels in each layer\n",
    "    num_conv_layers=3,                      # Number of convolutional layers\n",
    "    num_linear_layers=2,                    # Number of linear layers\n",
    "    output_dim=32,                          # Output dimension of the embedding network\n",
    "    kernel_size=3,                          # Kernel size of the convolutional layers\n",
    "    pool_kernel_size=2                      # Kernel size of the pooling layers\n",
    ")\n",
    "\n",
    "# Define density estimator\n",
    "density_estimator= posterior_nn(\n",
    "    model='maf',                           # Model type: 'maf', 'nsf', 'mdn', etc.\n",
    "    hidden_features=32,                    # Number of hidden features in the MAF\n",
    "    num_transforms=10,                     # Number of transformations in the MAF\n",
    "    embedding_net=embedding_net            # Embedding network\n",
    ")\n",
    "\n",
    "# Define the NPE inference object\n",
    "npe = NPE(density_estimator=density_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training ===\n",
    "\n",
    "# Pass simulated data to NPE inference object\n",
    "inference = npe.append_simulations(train_theta, train_data)\n",
    "\n",
    "# Train the posterior estimator\n",
    "posterior_estimator = inference.train(\n",
    "    training_batch_size=64,                 # Batch size for training\n",
    "    learning_rate=1e-3,                     # Learning rate for training\n",
    "    validation_fraction=0.1,                # Fraction of data to use for validation\n",
    "    max_num_epochs=50                       # Maximum number of epochs for training \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training Visualization===\n",
    "\n",
    "# Plot loss curve\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(inference.summary['training_loss'], label='Training Loss', color='blue', linestyle='-')\n",
    "ax.plot(inference.summary['validation_loss'], label='Validation Loss', color='red', linestyle='--')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inference ===\n",
    "\n",
    "# Build the posterior\n",
    "posterior = inference.build_posterior(posterior_estimator)\n",
    "\n",
    "# Perform inference on test set\n",
    "n_samples = 500 # Number of samples to draw from the posterior\n",
    "samples_theta = posterior.sample_batched((n_samples,), x = test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the posterior\n",
    "test_example_index = 7 # TODO: Change this to the index of the test example you want to visualize\n",
    "test_example_data = test_data[test_example_index]\n",
    "test_example_theta = test_theta[test_example_index]\n",
    "test_example_samples = samples_theta[:, test_example_index]\n",
    "\n",
    "fig = corner.corner(\n",
    "        np.array(test_example_samples),\n",
    "        labels=parameter_names,\n",
    "        truths=test_example_theta,\n",
    "        truth_color='k',\n",
    "        levels=[0.68, 0.95],\n",
    "        bins=20,\n",
    "        plot_datapoints=False,\n",
    "        fill_contours=True,\n",
    "        max_n_ticks=3,\n",
    "        smooth=1.0,\n",
    "        hist_kwargs=dict(density=True, color='green', linewidth=2, histtype='step'),\n",
    "        color='green',\n",
    "        title_fmt='.2f',\n",
    "        title_kwargs=dict(fontsize=20),\n",
    "        label_kwargs=dict(fontsize=20),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Calibration Testing ===\n",
    "\n",
    "# Compute TARP coverage\n",
    "ecp, alpha = get_tarp_coverage(\n",
    "    np.array(samples_theta), \n",
    "    np.array(test_theta),\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "# Plot coverage\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot([0, 1], [0, 1], color='black', linestyle='--', label='Perfect Calibration')\n",
    "ax.plot(ecp, alpha, color='orange', label='TARP Coverage')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Empirical Coverage Probability')\n",
    "ax.set_ylabel('Credibility Level')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parity Testing === \n",
    "\n",
    "# # Choose parameter to generate parity plot for\n",
    "# parameter_index = 9 # TODO: Change this to the index of the parameter you want to generate a parity plot for\n",
    "# theta_true = np.array(test_theta)[:, parameter_index]\n",
    "# theta_predicted = np.mean(np.array(samples_theta)[..., parameter_index], axis=0)\n",
    "# theta_error = np.std(np.array(samples_theta)[..., parameter_index], axis=0)\n",
    "\n",
    "# # Parity plot\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# ax.plot([np.min(theta_true), np.max(theta_true)], [np.min(theta_true), np.max(theta_true)], color='black', linestyle='--', label='Perfect Prediction')\n",
    "# ax.errorbar(theta_predicted, theta_true, yerr=theta_error, fmt='.', color='purple', label='Predicted vs True')\n",
    "# ax.set_xlabel('Predicted Value')\n",
    "# ax.set_ylabel('True Value')\n",
    "# ax.set_title('Parity Plot')\n",
    "# ax.set_title(f'Parity Plot for {parameter_names[parameter_index]}')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slsim-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
